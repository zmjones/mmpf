
for reviewer 1.

there is a bit of literature on treatment effect heterogeneity that seems very similar to what you suggest. i added a paragraph which discusses potential applications of these methods and i reference some papers in this literature there. the consensus amongst the econometricians and statisticians of whose work i am aware of seems to be that regularization bias can be a significant problem, and to avoid this ml has to be tuned a bit differently, used in a multi-step process which removes this bias, or formulated differently (i.e., by predicting treatment effects rather than outcomes). really seemingly all of these things. discussing all of this i think would be too much (beyond the scope of the paper), so i felt it was appropriate to instead just reference some of the literature, as i haven't made a contribution to ml/causal inference at all.

since mmpf can operate on prediction function which produce a vector of outputs for each input (e.g., class probabilities) it can also operate on summaries of the conditional distribution of the outcome which are also vector valued. so you could do quantile regression and then compute partial dependence using mmpf. thus it will work in the case where your prediction model's output is a expected value and a variance or standard error. in that sense i can propagate estimates of sampling uncertainty based on the underlying model. i've now added a sentence mentioning this. in some of the software i've built that uses mmpf this is more of a built-out feature (e.g. in mlr). i don't currently have a way to represent uncertainty from the additional approximation error induced by using the mmpf approximation though. that is something i am working on now as a part of a larger project.

for reviewer 2.

1. i added some appropriate references.
1.2. i don't think that is specifically correct. for example suppose we have data from a two parameter linear-additive model y = x * beta + epsilon_w where when w = 1 epsilon is 1 and when it is 0 epsilon is 10. if you fit a model of the form y | x = x * beta_1 i(w = 1) + x * beta_2 i(w = 0) where i(.) is the indicator function, and took the derivative w.r.t. x you'd get beta_1 i(w = 1) + beta_2 i(w = 0). the expectation of this is just beta. i know that is very basic but all i'm saying is that heteroskedasticity can screw up inference which depends on homoskedasticity, but almost by definition it can't screw up the mean function estimation, which is what all of this is about. asymptotics aside i am sure you could get unlucky and have heteroskedasticity look like first order misspecification (i tend to think they both occur simultaneously in practice), but as an exploratory diagnostic i don't think it is problematic for this reason. in any case what you seem to be focused on is the ability of an underlying model to be biased because of heteroskedasticity (i.e. confusing unequal variance for unequal means) which is certainly possible. i view it as advantageous that a diagnostic would faithfully represent that the model learned that to the user. i did some simulations using the simulation i used for this paper and at least in that simple case my intuition appears to be right. heteroskedasticity can make integrating against the joint distribution much more distorting as well.

2.1. i use mmpf in two separate packages (edarf and mlr) that do exactly this, but i don't think this fits here. this was intended to be lightweight, have minimal dependencies, and provide the user with maximum control. it isn't really usable if you don't have some programming ability and i think it is reasonable to expect the user to be able to make basic plots, or they can use one of the more end-to-end solutions i mentioned.

2.2. in this scenario x2 is indirectly related to f of course. the way i've (and giles hooker and also david friedman) have posed it the goal is the accurate (faithful to the fit model) recovery of additive components. to them i guess it is obvious that integrating against the joint distribution is misleading, and my little simulation just highlights that. so in that sense i'm not sure why you'd want to integrate against the joint distribution even if you had it (like i do in the simulation). 

friedman discusses using an estimate of  p(x_u | x_{-u}) in his 2001 paper (and dismisses it for the first reason), and hooker (2007) sort of indirectly discusses it by discussing a symptom of integrating against the marginal distribution, which is that the estimates of the additive components can, in some scenarios, depend on functional behavior in regions of low density (especially in high dimensions). i.e. if there is strong dependence between x_u and x_{-u} this can mean that their joint distribution is very non-uniform, and what a model learned on such data says in those empty regions depends much more on the model class than on the data.

in any case you can already do what you suggest by using the weights argument to marginalPrediction (which is how i setup the simulation to show how this is a bad idea). 
